# Agente LLM para Ticketing

Un agente inteligente que combina **consultas SQL**, **b√∫squeda web** y
**razonamiento natural** usando un **modelo LLM local o remoto** con API
tipo OpenAI.

## Estructura del proyecto

    agent_project/
    ‚îÇ
    ‚îú‚îÄ‚îÄ backend/
    ‚îÇ   ‚îú‚îÄ‚îÄ main.py
    ‚îÇ   ‚îú‚îÄ‚îÄ router.py
    ‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py
    ‚îÇ   ‚îú‚îÄ‚îÄ db_client.py
    ‚îÇ   ‚îú‚îÄ‚îÄ web_search.py
    ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py
    ‚îÇ   ‚îî‚îÄ‚îÄ config.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ frontend/
    ‚îÇ   ‚îî‚îÄ‚îÄ app.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ tickets.db
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îî‚îÄ‚îÄ README.md

## Instalaci√≥n

### 1. Crear entorno virtual

``` bash
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
.venv\Scripts\activate    # Windows
```

### 2. Instalar dependencias

``` bash
pip install -r requirements.txt
```

### 3. Configurar .env

    LLM_PROVIDER="local"  # "openai" | "gemini" | "local" | "lmstudio"

    LLM_API_BASE=http://localhost:8001/v1
    LLM_MODEL=model_name

    LLM_API_KEY=""
    OPENAI_API_KEY=""
    GEMINI_API_KEY=""

    TAVILY_API_KEY=""

    DB_PATH=./tickets.db

### 4. Levantar backend

``` bash
uvicorn backend.main:app --reload --port 8000
```

### 5. Levantar frontend

``` bash
streamlit run frontend/app.py
```

### 6. üß† C√≥mo funciona el agente internamente

1. User ‚Üí mensaje textual
2. FastAPI ‚Üí Router decide intenci√≥n via LLM
3. Seg√∫n la intenci√≥n:
   - **sql** ‚Üí genera SQL, ejecuta, resume resultados
   - **web** ‚Üí busca info con Tavily, resume, devuelve fuentes
   - **llm** ‚Üí respuesta directa del modelo
4. Devue lve JSON a Streamlit
5. Streamlit muestra la respuesta y renderiza tablas o fuentes


## Ejemplos de uso

### Consulta SQL

> "Mu√©strame los tickets abiertos."

### Web search

> "¬øCu√°les son los trending topics?"

### Respuesta LLM

> "Explica qu√© es un SLA."
